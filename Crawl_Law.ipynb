{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0dd0a-159b-402b-a147-5008e4184141",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai requests pillow selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5a1c9-fa71-4282-bb59-39555eca8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import urllib.parse\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "import re\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "image_cache: dict[str, str] = {}\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY\")\n",
    " )\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException,\n",
    "    TimeoutException,\n",
    "    InvalidSessionIdException,\n",
    "    WebDriverException,\n",
    "    StaleElementReferenceException\n",
    ")\n",
    "\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', '_', name)\n",
    "\n",
    "def scroll_to_bottom(driver, pause_sec=1.5, max_scroll=30):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_count = 0\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause_sec)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        scroll_count += 1\n",
    "        if scroll_count >= max_scroll:\n",
    "            break\n",
    "\n",
    "def scroll_inside_div(driver, div_selector=\"#contentBody\", pause_sec=1.5, max_scroll=30):\n",
    "    try:\n",
    "        container = driver.find_element(By.CSS_SELECTOR, div_selector)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"[scroll_inside_div] {div_selector} not found.\")\n",
    "        return\n",
    "\n",
    "    last_scroll_height = driver.execute_script(\"return arguments[0].scrollHeight;\", container)\n",
    "    scroll_count = 0\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight;\", container)\n",
    "        time.sleep(pause_sec)\n",
    "\n",
    "        new_scroll_height = driver.execute_script(\"return arguments[0].scrollHeight;\", container)\n",
    "        if new_scroll_height == last_scroll_height:\n",
    "            break\n",
    "\n",
    "        last_scroll_height = new_scroll_height\n",
    "        scroll_count += 1\n",
    "        if scroll_count >= max_scroll:\n",
    "            break\n",
    "\n",
    "def get_page_title_via_h2(url) -> str:\n",
    "    chrome_driver_path = \"YOUR_CHROMEDRIVER_PATH\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('headless')\n",
    "    options.add_argument('disable-gpu')\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    title_text = \"\"\n",
    "    try:\n",
    "        h2_el = driver.find_element(By.CSS_SELECTOR, \"div#conTop h2\")\n",
    "        title_text = h2_el.text.strip()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"[get_page_title_via_h2] Exception occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return title_text\n",
    "\n",
    "def _smart_scroll(driver, sel=\"#conScroll\", pause=1.0, max_round=20):\n",
    "    box = None\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        box = driver.find_element(By.CSS_SELECTOR, sel)\n",
    "    except NoSuchElementException:\n",
    "        print(f\"[WARN] {_smart_scroll.__name__}: {sel} not found, retrying in 2s...\")\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            box = driver.find_element(By.CSS_SELECTOR, sel)\n",
    "        except NoSuchElementException:\n",
    "            print(f\"[FAIL] {_smart_scroll.__name__}: {sel} still not found after retry\")\n",
    "            return\n",
    "\n",
    "    same_cnt = 0\n",
    "    last = driver.execute_script(\"return arguments[0].scrollHeight\", box)\n",
    "\n",
    "    for _ in range(max_round):\n",
    "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", box)\n",
    "        time.sleep(pause)\n",
    "        cur = driver.execute_script(\"return arguments[0].scrollHeight\", box)\n",
    "        if cur == last:\n",
    "            same_cnt += 1\n",
    "        else:\n",
    "            same_cnt = 0\n",
    "            last = cur\n",
    "        if same_cnt >= 2:  \n",
    "            break\n",
    "\n",
    "def _looks_like_revision(seg: str) -> bool:\n",
    "    seg = seg.strip()\n",
    "    if seg.startswith(\"(\") and seg.endswith(\")\"):\n",
    "        seg = seg[1:-1]\n",
    "    return bool(seg) and re.fullmatch(r\"[0-9,\\- ]+\", seg) is not None\n",
    "\n",
    "def extract_law_name_from_url(url: str) -> str:\n",
    "    parts = [urllib.parse.unquote(p) for p in urllib.parse.urlparse(url).path.split(\"/\") if p]\n",
    "    for seg in reversed(parts):\n",
    "        if not _looks_like_revision(seg):\n",
    "            return sanitize_filename(seg)\n",
    "    return sanitize_filename(parts[-1]) if parts else \"law\"\n",
    "\n",
    "def parse_image_with_gpt(image_url: str) -> str:\n",
    "    if \"button\" in image_url.lower():\n",
    "        print(f\"[parse_image_with_gpt] 'button' in URL => Skip: {image_url}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=10, verify=False)\n",
    "        response.raise_for_status()\n",
    "        img_data = response.content\n",
    "\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
    "\n",
    "        if \"gif\" in content_type or image_url.lower().endswith('.gif'):\n",
    "            try:\n",
    "                pil_img = Image.open(BytesIO(img_data))\n",
    "                pil_img = pil_img.convert(\"RGB\")\n",
    "                buffer = BytesIO()\n",
    "                pil_img.save(buffer, format=\"JPEG\")\n",
    "                converted_data = buffer.getvalue()\n",
    "                print(f\"[parse_image_with_gpt] GIF->JPG Conversion Complete: {image_url}\")\n",
    "                ext = \"jpg\"\n",
    "                encoded = base64.b64encode(converted_data).decode(\"utf-8\")\n",
    "            except Exception as e:\n",
    "                print(f\"[parse_image_with_gpt] GIF Conversion Failed, skip: {e}\")\n",
    "                return \"\"\n",
    "        else:\n",
    "            if \"png\" in content_type:\n",
    "                ext = \"png\"\n",
    "            else:\n",
    "                ext = \"jpg\"\n",
    "\n",
    "            encoded = base64.b64encode(img_data).decode(\"utf-8\")\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "              \"role\": \"system\",\n",
    "              \"content\": \"You are an expert OCR and table-to-text conversion agent. Your job is to convert any image—especially tables or diagrams—into clean, plain text. You must answer in Korean. Do not output any explanation, disclaimer, or commentary. Just the converted text, in Korean.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"The image below may be a table or a diagram. Please convert it into text form. Do not say anything else—only output the converted text. You must answer in Korean.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/{ext};base64,{encoded}\"\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return completion.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[parse_image_with_gpt] Image Processing Error Occurred: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "from selenium.common.exceptions import NoSuchFrameException\n",
    "\n",
    "def switch_into_content_frame(driver, wait_sec: int = 10) -> bool:\n",
    "    try:\n",
    "        WebDriverWait(driver, wait_sec).until(\n",
    "             lambda d: d.find_elements(By.ID, \"contentBody\")\n",
    "                   or d.find_elements(By.TAG_NAME, \"iframe\")\n",
    "         )\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "    frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "    if not frames:\n",
    "        return False            \n",
    "\n",
    "    frame_ids = [\"lawService\", \"admRulService\", \"viewer\", \"viewFrame\"]\n",
    "\n",
    "    for fid in frame_ids:\n",
    "        try:\n",
    "            driver.switch_to.frame(fid)\n",
    "            return True\n",
    "        except NoSuchFrameException:\n",
    "            continue\n",
    "\n",
    "    driver.switch_to.frame(frames[0])\n",
    "    return True\n",
    "\n",
    "def detect_sections(url):\n",
    "    try:\n",
    "        sections, pgroup_count = detect_sections_code1(url)\n",
    "        if not sections or len(sections) < 2:\n",
    "            raise ValueError(\"code1 detect_sections results are not normal.\")\n",
    "        print(\"[detect_sections] Sections are divided by Code 1.\")\n",
    "        return sections, pgroup_count\n",
    "    except Exception as e:\n",
    "        print(f\"[detect_sections] Code 1 Failed. Fallback to Code 2: {e}\")\n",
    "\n",
    "    sections, pgroup_count = detect_sections_code2(url)\n",
    "    return sections, pgroup_count\n",
    "\n",
    "def detect_sections_code1(url):\n",
    "    chrome_driver_path = \"YOUR_CHROMEDRIVER_PATH\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('headless')\n",
    "    options.add_argument('disable-gpu')\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "    sections = []\n",
    "    pgroup_count = 0\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        switch_into_content_frame(driver) \n",
    "        time.sleep(1)\n",
    "        _smart_scroll(driver, \"#conScroll\")\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.pgroup\"))\n",
    "        )\n",
    "        pgroup_list = driver.find_elements(By.CSS_SELECTOR, \"div.pgroup\")\n",
    "        pgroup_count = len(pgroup_list)\n",
    "        for i, pg in enumerate(pgroup_list):\n",
    "            has_chapter = pg.find_elements(By.CSS_SELECTOR, \"p.gtit\")\n",
    "            has_buchik = pg.find_elements(By.CSS_SELECTOR, \"p.pty3\")\n",
    "            if has_chapter:\n",
    "                sections.append((i, \"chapter\"))\n",
    "            elif has_buchik:\n",
    "                sections.append((i, \"buchik\"))\n",
    "            else:\n",
    "                sections.append((i, \"chapter\"))\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    if not sections:\n",
    "        sections = [(0, \"chapter\")]\n",
    "        pgroup_count = 1\n",
    "\n",
    "    sections.append((pgroup_count, \"\"))\n",
    "    return sections, pgroup_count\n",
    "\n",
    "def detect_sections_code2(url):\n",
    "    chrome_driver_path = \"YOUR_CHROMEDRIVER_PATH\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"headless\")\n",
    "    options.add_argument(\"disable-gpu\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "    sections = []\n",
    "    pgroup_count = 0\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        switch_into_content_frame(driver)   \n",
    "        time.sleep(1)\n",
    "        \n",
    "        _smart_scroll(driver, \"#conScroll\")\n",
    "\n",
    "        pgroup_list = driver.find_elements(By.CSS_SELECTOR, \"div.pgroup\")\n",
    "        pgroup_count = len(pgroup_list)\n",
    "        print(f\"[detect_sections_code2] found pgroup_count={pgroup_count}\")\n",
    "\n",
    "        if pgroup_count == 0:\n",
    "            sections = [(0, \"chapter\")]\n",
    "            pgroup_count = 1\n",
    "        else:\n",
    "            for i, pg in enumerate(pgroup_list):\n",
    "                has_chapter = pg.find_elements(By.CSS_SELECTOR, \"p.gtit\")\n",
    "                has_buchik = pg.find_elements(By.CSS_SELECTOR, \"p.pty3\")\n",
    "                if has_chapter:\n",
    "                    sections.append((i, \"chapter\"))\n",
    "                elif has_buchik:\n",
    "                    sections.append((i, \"buchik\"))\n",
    "                else:\n",
    "                    sections.append((i, \"chapter\"))\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    sections.append((pgroup_count, \"\"))\n",
    "    return sections, pgroup_count\n",
    "\n",
    "from selenium.common.exceptions import NoSuchFrameException\n",
    "\n",
    "class SectionCrawler:\n",
    "    def __init__(self, url, section_type, section_num, law_name,\n",
    "                 pgroup_list, start_idx, end_idx, page_len, \n",
    "                 skip_popup_text=\"There is no subordinate legislation stipulating the matters delegated by the statute.\"):\n",
    "        self.url = url\n",
    "        self.section_type = section_type\n",
    "        self.section_num = section_num\n",
    "        self.law_name = law_name\n",
    "        self.pgroup_list = pgroup_list\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "        self.skip_popup_text = skip_popup_text\n",
    "        \n",
    "        self.image_cache = image_cache\n",
    "        \n",
    "        self.page_len = page_len\n",
    "        self.page_len_threshold = int(page_len * 0.90)   \n",
    "        self._single_saved = False\n",
    "\n",
    "        self.failed_links = set()\n",
    "        self.visited_links = set()  \n",
    "        self.results = []\n",
    "        self.current_title = f\"{section_type.upper()}_{section_num}\"\n",
    "\n",
    "    def run(self):\n",
    "        pcount = len(self.pgroup_list)\n",
    "        if self.end_idx > pcount:\n",
    "            self.end_idx = pcount\n",
    "\n",
    "        i = self.start_idx\n",
    "        while i < self.end_idx:\n",
    "            if self._single_saved:      \n",
    "                break\n",
    "            if i >= pcount:\n",
    "                break\n",
    "            print(f\"   -> pgroup {i} / {self.section_type}:{self.section_num} starting...\")\n",
    "\n",
    "            try:\n",
    "                self.driver = self._new_driver()\n",
    "                _smart_scroll(self.driver, \"#conScroll\")\n",
    "\n",
    "                all_pgroups = self.driver.find_elements(By.CSS_SELECTOR, \"div.pgroup\")\n",
    "                if i >= len(all_pgroups):\n",
    "                    self.driver.quit()\n",
    "                    break\n",
    "\n",
    "                pg = all_pgroups[i]\n",
    "\n",
    "                if i == self.start_idx:\n",
    "                    gtit = pg.find_elements(By.CSS_SELECTOR, \"p.gtit\")\n",
    "                    if gtit:\n",
    "                        self.current_title = gtit[0].text.strip()\n",
    "\n",
    "                self._process_chapter_pgroup(i, pg)\n",
    "\n",
    "                self.driver.quit()\n",
    "                i += 1\n",
    "\n",
    "            except WebDriverException as ex_drv:\n",
    "                print(f\"   -> WebDriverException occurred but pgroup {i} still continue: {ex_drv}\")\n",
    "\n",
    "                try:\n",
    "                    self.driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "                #i += 1\n",
    "                continue\n",
    "\n",
    "            except Exception as ex:\n",
    "                print(f\"   -> skip pgroup {i} due to error: {ex}\")\n",
    "                try:\n",
    "                    self.driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "                i += 1\n",
    "\n",
    "        return self.results\n",
    "\n",
    "\n",
    "    def _new_driver(self):\n",
    "        chrome_driver_path = \"YOUR_CHROMEDRIVER_PATH\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"headless\")\n",
    "        options.add_argument(\"disable-gpu\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-software-rasterizer\")\n",
    "        options.page_load_strategy = 'eager'\n",
    "        drv = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "        drv.set_page_load_timeout(120)\n",
    "        drv.get(self.url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        switch_into_content_frame(drv)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        _smart_scroll(drv, \"#conScroll\")\n",
    "\n",
    "        return drv\n",
    "\n",
    "    def _process_chapter_pgroup(self, pgroup_idx, pgroup_elem):\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 5).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"p.pty1_p4\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(f\"     => no p.pty1_p4 in pgroup {pgroup_idx}\")\n",
    "\n",
    "        article_elems = pgroup_elem.find_elements(By.CSS_SELECTOR, \"p.pty1_p4\")\n",
    "        print(f\"     => pgroup {pgroup_idx} article count = {len(article_elems)}\")\n",
    "\n",
    "        for art_j, art in enumerate(article_elems):\n",
    "            if self._single_saved:\n",
    "                break\n",
    "            self._process_article(pgroup_idx, art_j, art)\n",
    "\n",
    "    def _process_article(self, pgroup_idx, art_j, art):\n",
    "        del_inputs = art.find_elements(By.XPATH, \".//input[starts-with(@id,'delJo') and @value='삭제']\")\n",
    "        is_deleted = (len(del_inputs) > 0) or (\"삭제\" in art.text)\n",
    "\n",
    "        try:\n",
    "            label_elem = art.find_element(By.TAG_NAME, \"label\")\n",
    "            article_title = label_elem.text.strip()\n",
    "        except NoSuchElementException:\n",
    "            article_title = art.text.strip()\n",
    "\n",
    "        if is_deleted:\n",
    "            pgroup_elem_parent = art.find_element(By.XPATH, \"./ancestor::div[@class='pgroup']\")\n",
    "            full_text = pgroup_elem_parent.text.strip()\n",
    "            related_articles = []\n",
    "        else:\n",
    "            pgroup_elem_parent = art.find_element(By.XPATH, \"./ancestor::div[@class='pgroup']\")\n",
    "            all_ps = pgroup_elem_parent.find_elements(\n",
    "                By.CSS_SELECTOR,\n",
    "                \"p.pty1_p4, p.pty1_de2_1, p.pty1_de2, p.pty1_de2h, p.pty1_p2, p.pty1_p3, p.pty1_de3\"\n",
    "            )\n",
    "            text_list = [p.text.strip() for p in all_ps]\n",
    "            full_text = \"\\n\".join(text_list).strip()\n",
    "\n",
    "            img_els = pgroup_elem_parent.find_elements(By.TAG_NAME, \"img\")\n",
    "            for img in pgroup_elem_parent.find_elements(By.TAG_NAME, \"img\"):\n",
    "                src = (img.get_attribute(\"src\") or \"\").strip()\n",
    "                if not src or src.lower().endswith(\".gif\"):\n",
    "                    continue\n",
    "                if src not in self.image_cache:\n",
    "                    self.image_cache[src] = parse_image_with_gpt(src)\n",
    "                if self.image_cache[src]:\n",
    "                    full_text += self.image_cache[src] + \"\\n-----------------------------\\n\"\n",
    "\n",
    "            link_els = pgroup_elem_parent.find_elements(By.CSS_SELECTOR, \"a.link\")\n",
    "            link_texts = []\n",
    "            for link_el in link_els:\n",
    "                link_txt = link_el.text.strip()\n",
    "                if link_txt:\n",
    "                    link_texts.append(link_txt)\n",
    "\n",
    "            related_articles = []\n",
    "            for txt in link_texts:\n",
    "                related_articles.append((txt, \"링크본문_미수집\"))  \n",
    "                \n",
    "        print(f\"full_text len = {len(full_text)}, \"\n",
    "              f\"threshold = {self.page_len_threshold}\")\n",
    "                \n",
    "        if not self._single_saved and len(full_text) >= self.page_len_threshold:\n",
    "            self.results = [{                    \n",
    "                \"law_name\": self.law_name,\n",
    "                \"chapter\": \"\",\n",
    "                \"article\": self.law_name,          \n",
    "                \"text\": full_text,\n",
    "                \"deleted\": False,\n",
    "                \"keywords\": [],\n",
    "                \"related_articles\": related_articles   \n",
    "            }]\n",
    "            self._single_saved = True             \n",
    "            return                                \n",
    "\n",
    "        self.results.append({\n",
    "            \"law_name\": self.law_name,\n",
    "            \"chapter\": self.current_title,\n",
    "            \"article\": article_title,\n",
    "            \"text\": full_text,\n",
    "            \"deleted\": is_deleted,\n",
    "            \"keywords\": [],\n",
    "            \"related_articles\": related_articles\n",
    "        })\n",
    "\n",
    "    def _collect_links_and_popup(self, pgroup_idx, art_j, pgroup_elem_parent):\n",
    "        related_articles = []\n",
    "        main_window = self.driver.current_window_handle\n",
    "\n",
    "        link_index = 0\n",
    "        while True:\n",
    "            try:\n",
    "                link_els = pgroup_elem_parent.find_elements(By.CSS_SELECTOR, \"a.link\")\n",
    "                if link_index >= len(link_els):\n",
    "                    break\n",
    "\n",
    "                link_el = link_els[link_index]\n",
    "                link_text = link_el.text.strip()\n",
    "                onclick_attr = link_el.get_attribute(\"onclick\") or \"\"\n",
    "\n",
    "                outer_html = link_el.get_attribute(\"outerHTML\")\n",
    "                print(f\"       [DEBUG] link_index={link_index}, text='{link_text}', onclick='{onclick_attr}'\")\n",
    "                print(f\"       [DEBUG] outerHTML: {outer_html}\")\n",
    "\n",
    "                unique_key = (link_text, onclick_attr.strip())\n",
    "                if unique_key in self.visited_links:\n",
    "                    print(f\"       -> link already visited: {unique_key} => skip\")\n",
    "                    link_index += 1\n",
    "                    continue\n",
    "                self.visited_links.add(unique_key)\n",
    "\n",
    "                skip_patterns = [\"joStmdPop\", \"fJoHstShow\", \"arView\", \"fncArLawPop\"]\n",
    "                if any(sp in onclick_attr for sp in skip_patterns):\n",
    "                    print(f\"       [DEBUG] skip_patterns matched => '{onclick_attr}'\")\n",
    "                    link_index += 1\n",
    "                    continue\n",
    "\n",
    "                target_fns = [\"fncLawPop\", \"fncLsLawPop\", \"fncLsPttnLinkPop\"]\n",
    "                if not any(fn in onclick_attr for fn in target_fns):\n",
    "                    print(f\"       [DEBUG] target_fns not matched => '{onclick_attr}'\")\n",
    "                    link_index += 1\n",
    "                    continue\n",
    "\n",
    "                if (pgroup_idx, art_j, link_text) in self.failed_links:\n",
    "                    print(f\"       [DEBUG] already in failed_links => {link_text}\")\n",
    "                    link_index += 1\n",
    "                    continue\n",
    "\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", link_el)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                print(f\"       [DEBUG] about to click link => text='{link_text}', onclick='{onclick_attr}'\")\n",
    "\n",
    "                popup_res = self._open_popup_and_get_text(link_el, link_text, main_window)\n",
    "                related_articles.append((link_text, popup_res))\n",
    "                link_index += 1\n",
    "\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            except StaleElementReferenceException:\n",
    "                print(\"       -> StaleElementReferenceException, trying to search link again...\")\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except WebDriverException as wde:\n",
    "                print(f\"       -> WebDriverException for link '{link_text}': {wde}\")\n",
    "                self.failed_links.add((pgroup_idx, art_j, link_text))\n",
    "\n",
    "                print(\"       -> re-initialize driver due to WebDriverException\")\n",
    "                try:\n",
    "                    self.driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "                self.driver = self._new_driver()\n",
    "\n",
    "                link_index += 1\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"       [ERROR] link '{link_text}' skipped – {type(e).__name__}: {e}\")\n",
    "                self.failed_links.add((pgroup_idx, art_j, link_text))\n",
    "                link_index += 1\n",
    "                continue         \n",
    "\n",
    "        return related_articles\n",
    "\n",
    "    def _open_popup_and_get_text(self, link_el, link_text, main_window_handle):\n",
    "        old_url = self.driver.current_url\n",
    "        before_handles = self.driver.window_handles\n",
    "\n",
    "        print(f\"         [DEBUG] _open_popup_and_get_text start => link_text='{link_text}', old_url='{old_url}'\")\n",
    "        try:\n",
    "            link_el.click()\n",
    "        except Exception as e:\n",
    "            print(f\"       -> link click exception for '{link_text}': {e}\")\n",
    "            return \"\"\n",
    "\n",
    "        start_wait = time.time()\n",
    "        popup_text = \"\"\n",
    "        new_window_opened = False\n",
    "        dom_popup_found = False\n",
    "        url_changed = False\n",
    "\n",
    "        print(\"         [DEBUG] waiting for new window/DOM popup/url change...\")\n",
    "\n",
    "        while time.time() - start_wait < 60:\n",
    "            after_handles = self.driver.window_handles\n",
    "            curr_url = self.driver.current_url\n",
    "\n",
    "            print(f\"         [DEBUG] elapsed={round(time.time()-start_wait,1)}s, handles={after_handles}, current_url={curr_url}\")\n",
    "\n",
    "            if len(after_handles) > len(before_handles):\n",
    "                new_window_opened = True\n",
    "                break\n",
    "            if self._dom_dialog_exists():\n",
    "                dom_popup_found = True\n",
    "                break\n",
    "            if curr_url != old_url:\n",
    "                url_changed = True\n",
    "                break\n",
    "            time.sleep(1)\n",
    "\n",
    "        if not (new_window_opened or dom_popup_found or url_changed):\n",
    "            print(f\"       -> No URL change or Popups in 60s -> skip link '{link_text}'\")\n",
    "            return \"\"\n",
    "        \n",
    "        if new_window_opened:\n",
    "            after_handles = self.driver.window_handles\n",
    "            new_handle = None\n",
    "            for h in after_handles:\n",
    "                if h not in before_handles:\n",
    "                    new_handle = h\n",
    "                    break\n",
    "            if not new_handle:\n",
    "                print(\"       -> cannot find new_handle???\")\n",
    "                return \"\"\n",
    "\n",
    "            self.driver.switch_to.window(new_handle)\n",
    "            try:\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                )\n",
    "                popup_text = self.driver.find_element(By.TAG_NAME, \"body\").text.strip()\n",
    "                wait_start = time.time()\n",
    "                while 'Loading data...' in popup_text and (time.time() - wait_start < 15):\n",
    "                    time.sleep(2)\n",
    "                    popup_text = self.driver.find_element(By.TAG_NAME, \"body\").text.strip()\n",
    "\n",
    "                if self.skip_popup_text in popup_text:\n",
    "                    print(\"       -> popup has skip text => ''\")\n",
    "                    popup_text = \"\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"       -> popup body read error: {e}\")\n",
    "                popup_text = \"\"\n",
    "\n",
    "            try:\n",
    "                self.driver.close()\n",
    "            except:\n",
    "                pass\n",
    "            self.driver.switch_to.window(main_window_handle)\n",
    "            time.sleep(1)\n",
    "\n",
    "            return popup_text\n",
    "\n",
    "        if dom_popup_found:\n",
    "            try:\n",
    "                dtext = self._close_dom_dialog()\n",
    "                if dtext and self.skip_popup_text in dtext:\n",
    "                    print(\"       -> dom popup skip text => ''\")\n",
    "                    dtext = \"\"\n",
    "                return dtext\n",
    "            except Exception as e:\n",
    "                print(f\"       -> dom popup read error: {e}\")\n",
    "                return \"\"\n",
    "\n",
    "        if url_changed:\n",
    "            try:\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                )\n",
    "                popup_text = self.driver.find_element(By.TAG_NAME, \"body\").text.strip()\n",
    "                wait_start = time.time()\n",
    "                while 'Loading data...' in popup_text and (time.time() - wait_start < 15):\n",
    "                    time.sleep(2)\n",
    "                    popup_text = self.driver.find_element(By.TAG_NAME, \"body\").text.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"       -> url_changed read error: {e}\")\n",
    "                popup_text = \"\"\n",
    "\n",
    "            try:\n",
    "                self.driver.back()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            return popup_text\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    def _dom_dialog_exists(self):\n",
    "        try:\n",
    "            self.driver.find_element(By.CSS_SELECTOR, \"div.ui-dialog[role='dialog']\")\n",
    "            return True\n",
    "        except NoSuchElementException:\n",
    "            return False\n",
    "\n",
    "    def _close_dom_dialog(self):\n",
    "        try:\n",
    "            dialog = self.driver.find_element(By.CSS_SELECTOR, \"div.ui-dialog[role='dialog']\")\n",
    "            dtext = dialog.text.strip()\n",
    "            close_btn = dialog.find_element(By.CSS_SELECTOR, \"a[onclick*='TempJoDeleLayer.hiddenTempLsLinkLayer']\")\n",
    "            close_btn.click()\n",
    "            time.sleep(0.5)\n",
    "            return dtext\n",
    "        except NoSuchElementException:\n",
    "            return \"\"\n",
    "        except WebDriverException as e:\n",
    "            print(f\"      -> fail to close DOM dialog: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "def _crawl_buchik_merged(url, pgroup_list, start_idx, end_idx, buchik_index, law_name):\n",
    "    chrome_driver_path = \"YOUR_CHROMEDRIVER_PATH\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('headless')\n",
    "    options.add_argument('disable-gpu')\n",
    "\n",
    "    merged_texts = []\n",
    "    title_text = f\"부칙 {buchik_index}\"\n",
    "    driver = None\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "        driver.get(url)\n",
    "        switch_into_content_frame(driver)\n",
    "        \n",
    "        _smart_scroll(driver, \"#conScroll\")\n",
    "\n",
    "        all_pgroups = driver.find_elements(By.CSS_SELECTOR, \"div.pgroup\")\n",
    "\n",
    "        if end_idx > len(all_pgroups):\n",
    "            end_idx = len(all_pgroups)\n",
    "\n",
    "        for idx in range(start_idx, end_idx):\n",
    "            if idx >= len(all_pgroups):\n",
    "                break\n",
    "            pg = all_pgroups[idx]\n",
    "\n",
    "            if idx == start_idx:\n",
    "                pty3_list = pg.find_elements(By.CSS_SELECTOR, \"p.pty3\")\n",
    "                if pty3_list:\n",
    "                    tmp_title = pty3_list[0].text.strip()\n",
    "                    if tmp_title:\n",
    "                        title_text = tmp_title\n",
    "\n",
    "            lines = pg.find_elements(By.CSS_SELECTOR, \"p.pty3, p.pty3_dep1, p.pty3_dep2\")\n",
    "            for line in lines:\n",
    "                txt = line.text.strip()\n",
    "                if txt:\n",
    "                    merged_texts.append(txt)\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "    full_text = \"\\n\".join(merged_texts).strip()\n",
    "    result = {\n",
    "        \"law_name\": law_name,\n",
    "        \"chapter\": title_text,\n",
    "        \"article\": title_text,\n",
    "        \"text\": full_text,\n",
    "        \"deleted\": False,\n",
    "        \"keywords\": [],\n",
    "        \"related_articles\": []\n",
    "    }\n",
    "    return [result]\n",
    "\n",
    "def single_section_with_retry(url, start_idx, end_idx, section_type, section_num, law_name,page_len, max_retry=3):\n",
    "    attempts = 0\n",
    "    final_data = []\n",
    "    while attempts < max_retry:\n",
    "        attempts += 1\n",
    "        print(f\"[{section_type}:{section_num}] Attempt {attempts}/{max_retry}\")\n",
    "\n",
    "        driver = None\n",
    "        try:\n",
    "            chrome_driver_path = \"YOUR_CHROMEDRIVER_PATH\"\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"headless\")\n",
    "            options.add_argument(\"disable-gpu\")\n",
    "\n",
    "            driver = webdriver.Chrome(service=Service(chrome_driver_path), options=options)\n",
    "            driver.get(url)\n",
    "            time.sleep(1)\n",
    "\n",
    "            switch_into_content_frame(driver)   \n",
    "            time.sleep(1)\n",
    "                      \n",
    "            _smart_scroll(driver, \"#conScroll\")\n",
    "\n",
    "            pgroup_list = driver.find_elements(By.CSS_SELECTOR, \"div.pgroup\")\n",
    "            driver.quit()\n",
    "\n",
    "            if section_type == \"chapter\":\n",
    "                crawler = SectionCrawler(\n",
    "                    url=url,\n",
    "                    section_type=section_type,\n",
    "                    section_num=section_num,\n",
    "                    law_name=law_name,\n",
    "                    pgroup_list=pgroup_list,\n",
    "                    start_idx=start_idx,\n",
    "                    end_idx=end_idx,\n",
    "                    page_len=page_len            \n",
    "                )\n",
    "                data = crawler.run()\n",
    "            else:  \n",
    "                data = _crawl_buchik_merged(\n",
    "                    url, pgroup_list, start_idx, end_idx, section_num, law_name\n",
    "                )\n",
    "\n",
    "            final_data = data\n",
    "            break\n",
    "\n",
    "        except InvalidSessionIdException as e:\n",
    "            print(f\" -> invalid session on attempt {attempts}, retry: {e}\")\n",
    "            if attempts == max_retry:\n",
    "                print(\"   => max retry reached for invalid session. skip.\")\n",
    "            else:\n",
    "                continue\n",
    "        except WebDriverException as e:\n",
    "            print(f\" -> WebDriverException on attempt {attempts}, retry: {e}\")\n",
    "            if attempts == max_retry:\n",
    "                print(\"   => max retry reached for WebDriverException. skip.\")\n",
    "            else:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\" -> unexpected error on attempt {attempts}, stop. {e}\")\n",
    "            break\n",
    "        finally:\n",
    "            if driver:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    return final_data\n",
    "\n",
    "def crawl_law(url, start_chapter=None, end_chapter=None):\n",
    "    chrome_driver_path = \"YOUR_CHROMEDRIVER_PATH\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('headless')\n",
    "    options.add_argument('disable-gpu')\n",
    "    with webdriver.Chrome(service=Service(chrome_driver_path), options=options) as drv:\n",
    "        drv.get(url)\n",
    "        switch_into_content_frame(drv)\n",
    "        _smart_scroll(drv, \"#conScroll\")\n",
    "        page_text_len = len(drv.find_element(By.ID, \"contentBody\").text)\n",
    "    h2_text = get_page_title_via_h2(url)\n",
    "    if h2_text:\n",
    "        law_name = sanitize_filename(h2_text)\n",
    "        print(f\" -> h2 based law_name: {law_name}\")\n",
    "    else:\n",
    "        law_name = extract_law_name_from_url(url)\n",
    "        print(f\" -> URL based law_name: {law_name}\")\n",
    "\n",
    "    os.makedirs(law_name, exist_ok=True)\n",
    "    sections, pgroup_count = detect_sections(url)\n",
    "\n",
    "    all_data = []\n",
    "    chapter_count = 0\n",
    "    buchik_count = 0\n",
    "\n",
    "    for i in range(len(sections) - 1):\n",
    "        start_idx, sec_type = sections[i]\n",
    "        end_idx, _ = sections[i + 1]\n",
    "        if not sec_type:\n",
    "            continue\n",
    "\n",
    "        if sec_type == \"chapter\":\n",
    "            chapter_count += 1\n",
    "\n",
    "            if start_chapter and chapter_count < start_chapter:\n",
    "                print(f\"   -> skip chapter {chapter_count}, < start_chapter({start_chapter})\")\n",
    "                continue\n",
    "            if end_chapter and chapter_count > end_chapter:\n",
    "                print(f\"   -> skip chapter {chapter_count}, > end_chapter({end_chapter})\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n===== CHAPTER {chapter_count} pgroup[{start_idx}~{end_idx-1}] =====\")\n",
    "            partial = single_section_with_retry(\n",
    "                url, start_idx, end_idx,\n",
    "                \"chapter\", chapter_count, law_name,\n",
    "                page_text_len        \n",
    "            )\n",
    "            all_data.extend(partial)\n",
    "\n",
    "            fname = os.path.join(law_name, f\"chapter_{chapter_count}.json\")\n",
    "            with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(partial, f, ensure_ascii=False, indent=4)\n",
    "            print(f\" -> {fname} saved, count={len(partial)}\")\n",
    "\n",
    "        else:\n",
    "            if start_chapter or end_chapter:\n",
    "                print(f\"\\n===== SKIP BUCHIK => (start_chapter={start_chapter}, end_chapter={end_chapter}) =====\")\n",
    "                continue\n",
    "\n",
    "            buchik_count += 1\n",
    "            print(f\"\\n===== BUCHIK {buchik_count} pgroup[{start_idx}~{end_idx-1}] =====\")\n",
    "            partial = single_section_with_retry(url, start_idx, end_idx, \"buchik\", buchik_count, law_name, None)\n",
    "            all_data.extend(partial)\n",
    "\n",
    "            fname = os.path.join(law_name, f\"buchik_{buchik_count}.json\")\n",
    "            with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(partial, f, ensure_ascii=False, indent=4)\n",
    "            print(f\" -> {fname} saved, count={len(partial)}\")\n",
    "\n",
    "    final_fname = os.path.join(law_name, \"final.json\")\n",
    "    with open(final_fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\n[{final_fname}] => {len(all_data)} results.\")\n",
    "\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5210b-79de-4c33-8ae6-80ab1cb8ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    url_list = [\n",
    "\n",
    "    ]\n",
    "\n",
    "    for url in url_list:\n",
    "\n",
    "        try:\n",
    "            data = crawl_law(url, start_chapter=None, end_chapter=None)\n",
    "            print(f\"[OK]  {url}  →  {len(data)} item(s)\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {url}\\n        {type(e).__name__}: {e}\")\n",
    "            continue           \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
